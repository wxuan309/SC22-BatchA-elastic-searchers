{"backend_state":"running","connection_file":"/projects/013ca607-9af4-4b8b-9eb4-c9fa83e65e2d/.local/share/jupyter/runtime/kernel-fa133446-44f6-49cf-8a59-d648803f7643.json","kernel":"python3-ubuntu","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"colab":{"name":"linear_regression_numpy_tutorial.ipynb","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1655226923578,"exec_count":1,"id":"7b7b75","input":"# import libraries \nimport numpy as np\nimport matplotlib.pyplot as plt","kernel":"python3-ubuntu","metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1627498066861,"user":{"displayName":"Fernanda Murillo","photoUrl":"","userId":"17562885266255337733"},"user_tz":600},"id":"IecuRdF1a-sG"},"pos":1,"start":1655226923575,"state":"done","type":"cell"}
{"cell_type":"code","end":1655226923591,"exec_count":2,"id":"35fbef","input":"# initialize your random seed to ensure reproducibility of your result\nnp.random.seed(42) # Why 42 (It works well for computer science) - https://medium.com/@leticia.b/the-story-of-seed-42-874953452b94\n\n# randomly generate x which is a vector of 100 points \nx = np.random.rand(1000, 1)\n\n# define exact linear function y = 1 + 2x + epsilon where epsilon (0.1*random numbers)\ny = 1 + 2 * x + .1 * np.random.randn(1000, 1)\nlen(y) ","kernel":"python3-ubuntu","metadata":{"executionInfo":{"elapsed":219,"status":"ok","timestamp":1627498568931,"user":{"displayName":"Fernanda Murillo","photoUrl":"","userId":"17562885266255337733"},"user_tz":600},"id":"BYHvay0xa-sK"},"output":{"0":{"data":{"text/plain":"1000"},"exec_count":2}},"pos":3,"start":1655226923581,"state":"done","type":"cell"}
{"cell_type":"code","end":1655226923608,"exec_count":3,"id":"5e4a41","input":"# Shuffles the indices\nidx = np.arange(1000) #will be 0-99, then will shuffel to random position with this:\n\nnp.random.shuffle(idx)\n\n# Uses first 80 random indices for train\ntrain_idx = idx[:800]\n\n# Uses the remaining indices for validation\nval_idx = idx[800:]\n\n# Generates train and validation sets\nx_train, y_train = x[train_idx], y[train_idx]\nx_val, y_val = x[val_idx], y[val_idx]\nidx ","kernel":"python3-ubuntu","metadata":{"executionInfo":{"elapsed":235,"status":"ok","timestamp":1627498579768,"user":{"displayName":"Fernanda Murillo","photoUrl":"","userId":"17562885266255337733"},"user_tz":600},"id":"_ntl2sX3a-sL"},"output":{"0":{"data":{"text/plain":"array([130, 839, 599, 511, 490, 529, 351,  36, 695, 704, 909, 971, 327,\n       275, 850, 621, 269, 179, 481, 260,  96, 560, 733, 242, 231, 917,\n       350, 762, 365, 843, 719, 860, 830, 503, 446, 517, 388, 835, 210,\n       690,  79, 609, 333, 545, 993, 536, 352, 337, 143, 674, 535, 868,\n       802, 990, 305, 505, 119,   1, 913, 976, 814, 184, 373, 166, 828,\n       460, 670, 577, 334, 284, 706, 206, 622, 524, 902, 981, 417,  31,\n       253,  41, 491, 471, 220,  71, 791, 955, 615, 623, 789, 616, 109,\n       151, 380, 150, 992, 785, 129, 978, 515, 682, 188, 659, 399, 694,\n       252, 584, 647, 421, 190, 937, 438,  50, 744, 723, 314,  84,  37,\n       149, 801, 681, 897, 968, 476, 547, 650,  59, 297, 717, 718, 462,\n        94, 747, 924, 867, 757, 315, 962, 361, 698, 157, 254, 582, 424,\n       964, 512, 316, 569, 824, 920, 594, 395, 270, 111, 781, 979, 178,\n       980, 996, 772, 105, 521, 425, 335, 710,  39, 196, 218, 329, 191,\n       662, 114, 344,  75, 808, 908,  55, 605, 926, 288, 367, 237, 144,\n       313, 596, 890, 509, 852, 494, 761, 202, 697, 894, 435,  57, 873,\n       655, 811, 708, 987, 819, 414, 250, 664, 870,  17, 985, 205, 466,\n       238, 925, 753, 716, 787, 885, 702, 809, 211,  66,  70, 774, 563,\n       995, 128,  33, 872, 803,   8, 732, 580, 477, 549, 274, 639, 258,\n       120, 946, 644, 555, 947, 175, 199, 393, 600, 826, 248, 942, 857,\n       422, 404, 991, 219, 514,  35, 929, 441, 497, 775, 308, 730, 635,\n       537, 696, 675, 865,  38,  93, 268, 530, 298, 246, 877, 845, 766,\n       347, 296, 739, 542, 640, 722,  88, 720, 346, 977, 590, 959, 457,\n       341, 699, 168,  69, 412,  10,  49, 928, 878, 318, 107, 456,   7,\n       997,   5, 468, 401, 853, 982, 973,  77, 506,  85,  61, 540, 805,\n       648, 767,  60, 844,  14, 366, 160, 585, 247, 307, 876,   4, 859,\n       281, 241, 919, 158,  46, 643, 804, 592, 777,  51, 798, 923, 737,\n       989, 405, 701, 338, 927, 630,  25, 198, 500, 915, 841, 525, 749,\n       463, 287, 142, 741, 634, 311, 132, 613, 370,  89,   3, 914, 763,\n        78, 958, 225,  97, 967, 861, 409, 131, 551, 714, 606, 216, 224,\n       472, 330, 561, 306, 692, 526, 134, 372,  42, 207, 489, 794, 649,\n       875, 243, 815,  54,  32, 921, 223, 390, 133, 693, 838, 221, 410,\n       186, 764, 180, 282, 185,  91, 618, 522, 271, 831, 118, 339, 579,\n       229, 345, 595, 896, 680, 432, 170,  28,  23, 371, 181, 975,  16,\n       544, 589, 869, 408, 641, 905, 397, 459, 953, 994, 724, 679, 881,\n       743,  45, 651, 999, 588,  43, 689, 137,  53, 822, 956, 856, 734,\n       502, 520, 251,  20, 355, 632, 846, 768, 467, 148, 576, 325, 874,\n         2, 273, 631, 262, 907, 676, 392, 598, 236, 172, 538, 898, 492,\n       480, 469, 793, 309, 458, 165, 984, 533,  15, 385, 571,  99, 948,\n       228, 884, 711, 745, 454, 597, 360, 235, 226, 461, 255, 145, 495,\n       642, 886, 829,  27, 728, 683, 501, 518, 292, 895, 531, 100, 591,\n       862, 568, 174, 217, 879, 400, 813, 197, 176, 871, 626, 451, 427,\n       136, 156, 797, 389,  87, 740, 208, 911,  95, 153, 240, 998, 965,\n       738, 587,  11, 465, 470, 755, 204, 614, 837, 653, 912, 661, 901,\n       177,  13, 159, 932, 483,   9,  73, 445, 543, 827, 951, 713, 790,\n       280, 312,  52, 402, 731, 436, 963, 320, 564, 230,  26, 566, 214,\n       833, 294, 607, 507, 721, 691, 705, 473, 888, 321, 608, 602, 820,\n       161, 840,  65, 527, 759, 122,  82, 572,  44, 887, 625, 933, 354,\n       782, 357, 413, 259, 383, 264, 534, 715, 646, 760, 256, 854, 758,\n       127, 836, 140, 629, 938, 765,  24, 291,  64, 783, 892, 326, 487,\n       899, 263, 369, 935, 807, 302,  56, 285, 546, 193, 792, 847, 800,\n       319, 565, 348,  21, 479, 113, 286, 945,  12, 784, 322, 574, 685,\n       336, 115, 328, 510,  47, 162, 742, 725,  81, 464, 727, 398, 406,\n       215, 416, 586, 101, 227, 848, 936, 303, 779, 146, 957, 581, 523,\n       940,  19, 183, 709, 475, 486, 834, 660, 245, 139, 810, 684, 266,\n       116,  90,  63, 903, 163, 612, 656, 516, 283, 922, 746, 663, 213,\n       103,  98, 272, 508, 434, 729, 110, 482, 636, 279, 603, 384, 498,\n       358, 138, 299, 532, 382, 972, 496, 773, 751, 415, 353, 553, 918,\n       378, 364, 673, 304, 488, 164,  68,  67, 277, 735, 931, 499, 637,\n       916,  58,  76, 368, 200, 707, 645,  86,  34, 123, 169, 960, 780,\n       121, 904, 541, 244, 124, 261, 883, 233, 447, 966, 429, 154, 665,\n       771, 939, 825, 578, 442, 961, 375, 433, 624, 387,  30, 485, 203,\n       817, 627, 687, 391, 324, 575, 394, 125, 474, 478, 426, 863, 443,\n       889, 668, 786, 147, 558, 192, 567, 420, 195,   6, 667, 439, 289,\n       678, 379, 611, 882, 671, 559,  74, 359,  22,  62, 893, 171, 155,\n       194, 450, 799, 363, 187,  29, 633,  72, 374, 428, 610, 943, 910,\n       106, 437, 276, 795, 293, 504, 407, 278, 222, 941, 983, 573, 212,\n       356, 812, 310, 832, 234, 418, 930,  80, 257, 295, 349, 386, 677,\n       604, 513, 628, 666, 453, 900, 770, 539, 949, 376, 712, 117, 562,\n       209, 265, 842, 954, 449, 201, 104, 796, 769, 950, 654, 754, 377,\n       952, 736, 112,  83, 411, 750, 970, 484, 593, 419, 638, 672, 323,\n       657, 849, 988, 686, 986, 726, 430, 669, 342, 173, 858, 152, 519,\n       452, 249, 102, 974, 300, 239, 444, 267, 108, 455, 583, 331,  18,\n       619, 167,   0, 556, 557, 126, 332, 906, 658, 493, 778, 969, 396,\n       550,  40, 182, 934, 570, 818, 788, 855, 528, 552, 816, 548, 343,\n       776, 232, 381, 821, 362, 617, 317, 703, 944, 756, 340, 806, 601,\n       864, 891, 403, 554, 135, 189, 880, 301,  48, 700, 652, 688, 823,\n       851, 748, 431, 290, 141, 423, 866, 752, 620,  92, 440, 448])"},"exec_count":3}},"pos":5,"start":1655226923594,"state":"done","type":"cell"}
{"cell_type":"code","end":1655226924105,"exec_count":4,"id":"d4f21a","input":"plt.figure(figsize=(10,5))\n\n# plot the train set \nplt.subplot(1,2,1)\nplt.scatter(x_train,y_train, c='orange')  \nplt.xlabel('x', fontsize = 20) \nplt.ylabel('y', fontsize = 20)\nplt.title('Generated Data - Train')\nplt.grid('on')\n\n# plot the validation set \nplt.subplot(1,2,2)\nplt.scatter(x_val,x_val)  \nplt.xlabel('x', fontsize = 20) \nplt.ylabel('y', fontsize = 20)\nplt.title('Generated Data - Test')\nplt.grid('on')\n\nplt.show()","kernel":"python3-ubuntu","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"executionInfo":{"elapsed":524,"status":"ok","timestamp":1627498583872,"user":{"displayName":"Fernanda Murillo","photoUrl":"","userId":"17562885266255337733"},"user_tz":600},"id":"W1JhyZ_Na-sM","outputId":"909f5fd9-c203-45cf-d41f-58b09bb727ef"},"output":{"0":{"data":{"image/png":"38c0e08df4354c34831fa8189a767878d970101f","text/plain":"<Figure size 720x360 with 2 Axes>"},"metadata":{"image/png":{"height":342,"width":620},"needs_background":"light"}}},"pos":7,"start":1655226923625,"state":"done","type":"cell"}
{"cell_type":"code","end":1655226924117,"exec_count":5,"id":"17b2b2","input":"# initialize your random seed to ensure reproducibility of your result\nnp.random.seed(42)\n\n# Initializes parameters \"a\" and \"b\" randomly - they don't need to be good yet\na = np.random.randn(1)\nb = np.random.randn(1)\n\n# print values of a and b \nprint(a, b)\n","kernel":"python3-ubuntu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1627498594869,"user":{"displayName":"Fernanda Murillo","photoUrl":"","userId":"17562885266255337733"},"user_tz":600},"id":"MjeQPrOKa-sP","outputId":"fe881c09-932e-46f0-9f40-79b6b5c69617"},"output":{"0":{"name":"stdout","text":"[0.49671415] [-0.1382643]\n"}},"pos":9,"start":1655226924115,"state":"done","type":"cell"}
{"cell_type":"code","end":1655226924126,"exec_count":6,"id":"a9caa4","input":"# Initialization of hyper-parameters (in our case, only learning rate and number of epochs)\n\n# Sets learning rate (how much the AI conforms to each set of feedbacks) --> asking the machine \"how much should i jump to the next step\"\nlr = 1e-1 \n# everytimethat you use a different out rig, there will be adifferent outcome\n\n# Defines number of epochs (how many times training is repeated)\nn_epochs = 1000","kernel":"python3-ubuntu","metadata":{"executionInfo":{"elapsed":240,"status":"ok","timestamp":1627498605594,"user":{"displayName":"Fernanda Murillo","photoUrl":"","userId":"17562885266255337733"},"user_tz":600},"id":"xaIPfjB1a-sP"},"pos":10,"start":1655226924125,"state":"done","type":"cell"}
{"cell_type":"code","end":1655226924183,"exec_count":7,"id":"ac055c","input":"for epoch in range(n_epochs):\n    # Computes our model's predicted output\n    yhat = a + b * x_train\n    \n    # How wrong is our model? That's the error! \n    error = (y_train - yhat)\n    \n    # It is a regression, so it computes mean squared error (MSE)\n    loss = (error ** 2).mean()\n    \n    # Computes gradients for both \"a\" and \"b\" parameters\n    a_grad = -2 * error.mean()\n    b_grad = -2 * (x_train * error).mean()\n    \n    # Updates parameters using gradients and the learning rate\n    a = a - lr * a_grad\n    b = b - lr * b_grad\n    \nprint(a, b)","kernel":"python3-ubuntu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":230,"status":"ok","timestamp":1627499222901,"user":{"displayName":"Fernanda Murillo","photoUrl":"","userId":"17562885266255337733"},"user_tz":600},"id":"IEGhbnhoa-sQ","outputId":"6df14990-801c-4030-bd27-e2c19b95c577"},"output":{"0":{"name":"stdout","text":"[1.01733799] [1.98435464]\n"}},"pos":12,"start":1655226924135,"state":"done","type":"cell"}
{"cell_type":"code","end":1655226924850,"exec_count":9,"id":"4417d9","input":"plt.figure(figsize=(10,5))\ny_vals = a + b * x_val\nplt.plot(x_val, y_vals, '--')\n\nplt.scatter(x_val,y_val, c='orange')  \nplt.xlabel('x', fontsize = 20) \nplt.ylabel('y', fontsize = 20)\nplt.title('Generated Data - Test')\nplt.grid('on')\nplt.show()","kernel":"python3-ubuntu","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1627499225902,"user":{"displayName":"Fernanda Murillo","photoUrl":"","userId":"17562885266255337733"},"user_tz":600},"id":"V9wnGeT_eou8","outputId":"1dc573e8-ebd4-4861-94e0-ff1a6d4229d3"},"output":{"0":{"data":{"image/png":"5cf1ccc18c34f512eb93edbf42cf9154d7978671","text/plain":"<Figure size 720x360 with 1 Axes>"},"metadata":{"image/png":{"height":342,"width":619},"needs_background":"light"}}},"pos":13,"start":1655226924537,"state":"done","type":"cell"}
{"cell_type":"code","end":1655226925283,"exec_count":10,"id":"3a532e","input":"from sklearn.linear_model import LinearRegression\nlinr = LinearRegression()\nlinr.fit(x_train, y_train)\nprint(linr.intercept_, linr.coef_[0])","kernel":"python3-ubuntu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":885,"status":"ok","timestamp":1627498630865,"user":{"displayName":"Fernanda Murillo","photoUrl":"","userId":"17562885266255337733"},"user_tz":600},"id":"jkPf0bt4a-sV","outputId":"422ab4c2-fceb-490e-ca2d-b45f09b26cff"},"output":{"0":{"name":"stdout","text":"[1.017337] [1.98435647]\n"}},"pos":15,"start":1655226924856,"state":"done","type":"cell"}
{"cell_type":"code","id":"0c8437","input":"","metadata":{"id":"bUuZFIRxa-sZ"},"pos":21,"type":"cell"}
{"cell_type":"code","id":"317617","input":"","metadata":{"id":"mdxlUKLka-sZ"},"pos":20,"type":"cell"}
{"cell_type":"code","id":"5967c4","input":"","metadata":{"id":"7ubqNNVxa-sZ"},"pos":22,"type":"cell"}
{"cell_type":"code","id":"7ff557","input":"","metadata":{"id":"8vkl-uJMa-sZ"},"pos":19,"type":"cell"}
{"cell_type":"code","id":"9e6316","input":"","metadata":{"id":"cvxyHdHha-sa"},"pos":24,"type":"cell"}
{"cell_type":"code","id":"ae68f4","input":"","metadata":{"id":"owTTVO4Ba-sY"},"pos":17,"type":"cell"}
{"cell_type":"code","id":"e06c38","input":"","metadata":{"id":"VPv4Rha8a-sa"},"pos":23,"type":"cell"}
{"cell_type":"code","id":"e65935","input":"","metadata":{"id":"l5xP_R3ya-sZ"},"pos":18,"type":"cell"}
{"cell_type":"code","id":"ec28b1","input":"","metadata":{"id":"ox6WlPb7a-sa"},"pos":25,"type":"cell"}
{"cell_type":"markdown","id":"2473e4","input":"# Linear regression using NumPy","metadata":{"id":"K_X2fB9ta-rv"},"pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"34d82b","input":"## The results!\n\nThey match up to 6 decimal places — we have a fully working implementation of linear regression using Numpy.","metadata":{"id":"yE4V3P9aa-sX"},"pos":16,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4c3000","input":"## linear regression using numpy ","metadata":{"id":"gi4pQP8Ia-sO"},"pos":8,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"74b9f8","input":"## check our results use Scikit-learn's linear regression\n\nJust to make sure we haven’t done any mistakes in our code, we can use Scikit-Learn’s Linear Regression to fit the model and compare the coefficients.\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html","metadata":{"id":"cbeaQMpna-sR"},"pos":14,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"80fbef","input":"## Data Generation","metadata":{"id":"JlrQHpg8a-sI"},"pos":2,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"f84451","input":"## split data into train and validation sets (80/20)","metadata":{"id":"irEOBQoca-sL"},"pos":4,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"fbc4f7","input":"### For each epoch, there are 5 training steps:\n* Compute model’s predictions \n* Compute the error (the difference between the actual value and predicted value) \n* Compute the loss ( mean square error = the average of (error)^2)\n* Compute the gradients for every parameter (require calculus)\n* Update the parameters a and b","metadata":{"id":"Tmk5nl_Va-sQ"},"pos":11,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"fe74ec","input":"## plot the train and validation sets","metadata":{"id":"DCCwyhkya-sM"},"pos":6,"state":"done","type":"cell"}
{"end":1655226924525,"exec_count":8,"id":"ee5360","input":"plt.figure(figsize=(10,5))\ny_pred = a + b * x_train\nplt.plot(x_train, y_pred, '--')\n\nplt.scatter(x_train,y_train, c='orange')  \nplt.xlabel('x', fontsize = 20) \nplt.ylabel('y', fontsize = 20)\nplt.title('Generated Data - Train')\nplt.grid('on')\nplt.show()","kernel":"python3-ubuntu","output":{"0":{"data":{"image/png":"18628dc2a6ebf52796e283a5e3c85ef6517dffdd","text/plain":"<Figure size 720x360 with 1 Axes>"},"metadata":{"image/png":{"height":342,"width":619},"needs_background":"light"}}},"pos":12.5,"start":1655226924189,"state":"done","type":"cell"}
{"id":0,"time":1655226869370,"type":"user"}
{"last_load":1655226798939,"type":"file"}